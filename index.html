<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      extensions: ["tex2jax.js"],
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
      tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
      TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
      messageStyle: "none"
    });
    </script>    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>

<html>


	<head>
		<title>Armand Gissler's homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="flag-icons-master/css/flag-icon.css">
		<link rel="stylesheet" href="css/academicons.min.css"/>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a class="logo"><h2>Armand Gissler</h2></a>
									<ul class="icons">
										<li><a href="https://scholar.google.com/citations?user=A_VULTAAAAAJ&hl" target=”_blank”><span class="ai ai-google-scholar-square ai-3x"></span></a></li>
										<!--<li><a href="https://orcid.org/0000-0003-3229-5715"><span class="ai ai-orcid-square ai-3x"></span></a></li>
										<li><a href="https://hal.science/search/index/?q=*&authFullName_s=Armand%20Gissler"><span class="ai ai-hal-square ai-3x"></span></a></li>
										<li><a href="https://arxiv.org/search/?searchtype=author&query=Gissler%2C+A"><span class="ai ai-arxiv-square ai-3x"></span></a></li>
										<li><a href="https://www.researchgate.net/profile/Armand-Gissler"><span class="ai ai-researchgate-square ai-3x"></span></a></li>-->
										<li><a href="CV.pdf" target=”_blank”><span class="ai ai-cv-square ai-3x"></span></a></li>
									</ul> 
								</header>

							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<p>I am a post-doctoral fellow in the <a href="https://sierra-mlopt.github.io/" target=”_blank”>Sierra</a>  team, in <a href="https://www.inria.fr/en/inria-paris-centre" target=”_blank”>Inria Paris</a>. 
										
											I currently work with <a href="https://www.di.ens.fr/~fbach/" target=”_blank”>Francis Bach</a> on sampling algorithms for diffusion models.</p>

										<p>I obtained my PhD in applied mathematics in <a href="https://www.polytechnique.edu/" target=”_blank”>École polytechnique</a>, while I worked in the <a href="https://team.inria.fr/randopt/" target=”_blank”>Randopt</a> <a href="https://www.inria.fr/" target=”_blank”>Inria</a> team, located at <a href="https://cmap.ip-paris.fr/" target=”_blank”>CMAP</a>. 
										
										My PhD advisors were <a href="http://www.cmap.polytechnique.fr/~anne.auger/" target=”_blank”>Anne Auger</a> and <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/" target=”_blank”>Nikolaus Hansen</a>.</p>
										
										During my PhD, I was working on theoretical aspects of the algorithm <a href="https://en.wikipedia.org/wiki/CMA-ES" target=”_blank”>CMA-ES</a> (Evolution Strategy with Covariance Matrix Adaptation), 
										specifically on a proof of convergence. 
									</div>

									
								</section>
								
							<section id="awards">
								<div class="content">
									<header class="major">
										<h2>Awards</h2>
									</header>
									<p><a href="https://fondation-hadamard.fr/en/our-programs/thematic-programs/pgmohome/phd-awards/">PGMO PhD Award 2025</a></p>
									<p><a href="https://sig.sigevo.org/SIGEVO-Dissertation-Award">SIGEVO Dissertation Award 2025</a>, Honorable Mention</p>
								</div>
								
							
							<!-- Publications -->
								<section id="publications">
									<div class="content">
										<header class="major">
											<h2>Publications</h2>	
										</header>

											<h3>Thesis</h3>
											<details><summary><a href="https://hal.science/tel-04935192v1/file/139589_GISSLER_2024_archivage.pdf" target=”_blank”>Linear convergence of evolution strategies with covariance matrix adaptation</a>, Armand Gissler, <i>Institut Polytechnique de Paris</i>, 2024.</summary>
												<div class="features">
												<article>
													<div class="content">
														<h5>Abstract: </h5>
														In optimization, CMA-ES is the
state-of-the-art algorithm among the evolution
strategies. Although it has found many
applications for more than 20 years, a
proof of its convergence remained an open
question. Solving this problem is the goal
of this thesis. More precisely, we provide
theoretical guarantees of linear convergence
of CMA-ES when minimizing an ellipsoidal
objective function. Moreover, we prove that
second-order information of a convex-quadratic
function is learnt since the covariance matrix
approximates the inverse Hessian matrix. To
establish our results, we normalize the states
of the algorithm and define a Markov chain
when the objective function is scaling-invariant.
The stability of this Markov chain proves then
the convergence of CMA-ES. We decompose
this proof in several steps. In the first two
chapters, we give a methodology to prove
the irreducibility and topological properties of
Markov chains that we apply to the normalized
Markov chain underlying CMA-ES. After the
next two chapters, we obtain that this Markov
chain is geometrically ergodic for ellipsoidal
problems. The last chapter concludes our proof.
													</div>
												</article>
												<article>
													<br>
													<span class="image object">
														<img width="300" src="images/cma.png" alt="" />
													</span>
												</article>
												</div>
											</details>
											<br>

											<h3>Preprints</h3>
											
											<details><summary><a href="https://inria.hal.science/hal-05514360v1/document" target=”_blank”>Adjusted Scores for Discrete Langevin Algorithms</a>, Armand Gissler, Saeed Saremi, Francis Bach, 2026.</summary>
												<div class="contents">
												<article>
														<h5>Abstract: </h5>
														Sampling from discrete distributions is a ubiquitous task in machine learning, recently revisited by the emergence of discrete diffusion models. While Langevin algorithms constitute the state of the art for continuous spaces, discrete versions lack similar theoretical guarantees when the step-size becomes small. In this paper, we address this limitation by interpreting discrete sampling algorithms as discretizations of continuous-time dynamics on the hypercube. In particular, we describe several score functions for discrete algorithms which result in approximations of Glauber dynamics for the correct target distribution. We also compute upper bounds for the contraction of these algorithms, with or without Metropolis adjustment. 
												</article>
												</div>
											</details>

											<details><summary><a href="https://inria.hal.science/hal-04713675v2/document" target=”_blank”>Irreducibility of nonsmooth state-space models with an application to CMA-ES</a>, Armand Gissler, Shan-Conrad Wolf, Anne Auger, Nikolaus Hansen, 2024 </summary>
												<div class="contents">
												<article>
														<h5>Abstract: </h5>
														We analyze a stochastic process resulting from the normalization of states in the zeroth-order optimization method CMA-ES. On a specific class of minimization problems where the objective function is scaling-invariant, this process defines a time-homogeneous Markov chain whose convergence at a geometric rate can imply the linear convergence of CMA-ES. However, the analysis of the intricate updates for this process constitute a great mathematical challenge. We establish that this Markov chain is an irreducible and aperiodic T-chain. These contributions represent a first major step for the convergence analysis towards a stationary distribution. We rely for this analysis on conditions for the irreducibility of nonsmooth state-space models on manifolds. To obtain our results, we extend these conditions to address the irreducibility in different hyperparameter settings that define different Markov chains, and to include nonsmooth state spaces. 
												</article>
												</div>
											</details>

											<details><summary><a href="https://arxiv.org/pdf/2402.06447.pdf" target=”_blank”>On the irreducibility and convergence of a class of nonsmooth nonlinear state-space models on manifolds</a>, Armand Gissler, Alain Durmus, Anne Auger, 2024 </summary>
												<div class="features">
												<article>
													<div class="content">
														<h5>Abstract: </h5>
														In this paper, we analyze a large class of general nonlinear state-space models on a state-space $\mathsf X$, defined by the recursion $\phi_{k+1}  = F(\phi_k,\alpha(\phi_k,U_{k+1}))$, $k \in\mathbb N$, where $F,\alpha$ are some functions and $\{U_{k+1}\}_{k\in\mathbb N}$ is a sequence of i.i.d. random variables.
														More precisely, we
														extend conditions under which this class of Markov chains is irreducible, aperiodic and satisfies important continuity properties, 
														relaxing two key assumptions from prior works. 
														First, the state-space $\mathsf X$ is supposed to be a smooth manifold instead of an open subset of a Euclidean space.
														Second, we only suppose that $F$ is locally Lipschitz continuous.
			
			
														We demonstrate the significance of our results through their application
														to Markov chains underlying optimization algorithms. 
														These schemes belong to the class of evolution strategies with covariance matrix adaptation and step-size adaptation.	
													</div>
												</article>
												<article>
													<br>
													<span class="image object">
														<img width="300" src="images/illustration-globally-attracting-states.png" alt="" />
													</span>
												</article>
												</div>
											</details>

											<br>

											<h3>Journal articles</h3>
											<details><summary><a href="https://inria.hal.science/hal-04386103v1/document" target=”_blank”>Asymptotic estimations of a perturbed symmetric eigenproblem</a>, Armand Gissler, Anne Auger, Nikolaus Hansen. <i>Applied Mathematics Letters</i>,
												Volume 150,
												2024. </summary>
												<div class="features">
												<article>
													<div class="content">
														<h5>Abstract: </h5>
														We study ill-conditioned positive definite matrices that are disturbed by the sum of
														rank-one matrices of a specific form. We provide estimates for the eigenvalues and eigenvectors. When the condition number of the initial matrix tends to infinity, we bound the values of the coordinates of the eigenvectors of the perturbed matrix. Equivalently, in the coordinate system where the initial matrix is diagonal, we bound the rate of convergence of coordinates that tend to zero.
													</div>
												</article>
												<article>
													<br>
													<span class="image object">
														<img width="300" src="images/eigenvector_last_coordinate_wrt_cond_number_multiD_multim.jpg" alt="" />
													</span>
												</article>
												</div>
											</details>
												
											<details><summary><a href="https://www.math.mcgill.ca/hoheisel/K_epi_rev3.pdf" target=”_blank”>A note on the K-epigraph</a>, Armand Gissler, Tim Hoheisel. <i>Optimization</i>, 72(9), 2251–2285, 2022. </summary>
												
												<div class="content">
													<h5>Abstract: </h5>
													We study the question as to when the closed convex hull of the graph of a K-convex map equals its K-epigraph. 
													In particular, we shed light onto the smallest cone K such that a given map has convex and closed K-epigraph, respectively. 
													We apply our findings to several examples in matrix space as well as to convex composite functions.
												</div>
											</details>
											<details><summary><a href="https://inria.hal.science/hal-03104436v2/file/scaling-invariance.pdf" target=”_blank”>Scaling-invariant functions versus positively homogeneous functions</a>, Cheikh Touré, Armand Gissler, Anne Auger, Nikolaus Hansen. <i>Journal of Optimization and Their Applications</i> (JOTA),  Volume 191, pages 363–383, 2021.
											</summary>
											<div class="features">
												<article>
													<div class="content">
														<h5>Abstract: </h5>
														Scaling-invariant functions preserve the order of points when the points are scaled by the same positive scalar (usually with respect to a unique reference point). 
														Composites of strictly monotonic functions with positively homogeneous functions are scaling-invariant with respect to zero. 
														We prove in this paper that also the reverse is true for large classes of scaling-invariant functions. 
														Specifically, we give necessary and sufficient conditions for scaling-invariant functions to be composites of a strictly monotonic function with a positively homogeneous function. 
														We also study sublevel sets of scaling-invariant functions generalizing well-known properties of positively homogeneous functions.	
													</div>
												</article>
												<article>
													<br>
													<span class="image object">
														<img width="300" src="images/scaling-invariant.png" alt="" />
													</span>
												</article>
												</div>
											</details>

											<br>

											<h3>Conference proceedings</h3>
											<details>
												<summary>
													<a href="https://hal.science/hal-04089923/file/evaluation2023author_version.pdf" target=”_blank”>Evaluation of the impact of various modifications to CMA-ES that facilitate its theoretical analysis</a>, Armand Gissler. <i>Proceedings of the Companion Conference on Genetic and Evolutionary Computation</i> (GECCO '23 Companion), Association for Computing Machinery, New York, NY, USA, 1603–1610, 2023.
												</summary>
												<div class="features">
													<article>
														<div class="content">
															<h5>Abstract: </h5>
															In this paper we introduce modified versions of CMA-ES with the objective to help to prove convergence of CMA-ES. In order to
															ensure that the modifications do not alter the performances of
															the algorithm too much, we benchmark variants of the algorithm
															derived from them on problems of the bbob test suite. We observe
															that the main performances losses are observed on ill-conditioned
															problems, which is probably due to the absence of cumulation in
															the adaptation of the covariance matrix. However, the versions of
															CMA-ES presented in this paper have globally similar performances
															to the original CMA-ES.	

															<a href="https://agissler.github.io/Benchmarking_proof_variants_CMA-ES/" target=”_blank”>Data</a>
														</div>
													</article>
													<article>
														<br>
														<span class="image object">
															<img width="350" src="images/benchmark.png" alt="" />
														</span>
													</article>
													</div>

											</details>
											<details>
												<summary>
													<a href="https://hal.inria.fr/hal-03644404/document" target=”_blank”>Learning rate adaptation by line search in evolution strategies with recombination</a>, Armand Gissler, Anne Auger, Nikolaus Hansen. <i>Proceedings of the Genetic and Evolutionary Computation Conference</i> (GECCO '22), Association for Computing Machinery, New York, NY, USA, 630–638, 2022.
												</summary>
												<div class="features">
													<article>
														<div class="content">
															<h5>Abstract: </h5>
															In this paper, we investigate the effect of a learning rate for the mean
															in Evolution Strategies with recombination. We study the effect of a
															half-line search after the mean shift direction is established, hence
															the learning rate value is conditioned to the direction. We prove
															convergence and study convergence rates in different dimensions
															and for different population sizes on the sphere function with the
															step-size proportional to the distance to the optimum. 

															We empirically find that a perfect half-line search increases the
															maximal convergence rate on the sphere function by up to about
															70%, assuming the line search imposes no additional costs. The
															speedup becomes less pronounced with increasing dimension. The
															line search reduces—however does not eliminate—the dependency
															of the convergence rate on the step-size. The optimal step-size
															assumes considerably smaller values with line search, which is
															consistent with previous results for different learning rate settings.
															The step-size difference is more pronounced in larger dimension
															and with larger population size, thereby diminishing an important
															advantage of a large population.
														</div>
													</article>
													<article>
														<br>
														<span class="image object">
															<img width="350" src="images/linesearch4.png" alt="" />
															<img width="350" src="images/linesearch8.png" alt="" />
														</span>
													</article>
													</div>
											</details>
					
										<p></p>
									</div>
								</section>
							
							<!-- Communications-->
								<section id="confs">
									<div class="content">
										<header class="major">
											<h2>Communications</h2>
										</header><div class="posts">
											<article>
												<a href="slides-ismp24.pdf" target=”_blank” class="image">
													<img src="images/cmaplot_divers_elli_cond106.png" alt="">
												</a>
												<div class="content">
													<h3>Convergence analysis of CMA-ES</h3>
													<p><a href="https://ismp2024.gerad.ca/" target=”_blank”>ISMP 2024</a>, July 2024, Montréal (Canada)</p>
												</div>
											</article>
											<article>
												<a href="slides-dagstuhl24.pdf" target=”_blank” class="image">
													<img src="images/elli-1.png" alt="">
												</a>
												<div class="content">
													<h3>Convergence proof of CMA-ES - Analysis of underlying Markov chains</h3>
													<p><a href="https://www.dagstuhl.de/24271" target=”_blank”>Dagstuhl seminar Theory of Randomized Optimization Heuristics</a>, July 2024, Dagstuhl (Germany)</p>
												</div>
											</article>
											<article>
												<a href="presentation-semdoc24.pdf" target=”_blank” class="image">
													<img src="images/linear_convergence_sphere_cmaes.png" alt="">
												</a>
												<div class="content">
													<h3>Convergence analysis of evolution strategies with covariance matrix adaptation</h3>
													<p><a href="https://cmap.ip-paris.fr/seminaires/seminaire-des-doctorants" target=”_blank”>Séminaire des doctorants CMAP-CMLS</a>, April 2024, Palaiseau (France)</p>
												</div>
											</article>
											<article>
												<a href="presentation-jps23.pdf" target=”_blank” class="image">
													<img src="images/attracting-states.png" alt="">
												</a>
												<div class="content">
													<h3>Irreducibility and convergence of nonlinear state-space models</h3>
													<p><a href="https://jps-2023.sciencesconf.org/" target=”_blank”>JPS 2023</a>, October 2023, Île d'Oléron (France)</p>
												</div>
											</article>
											<article>
												<a href="presentation-cjc23.pdf" target=”_blank” class="image">
													<img width="50" src="images/updated-covariance-rot-elli-1.png" alt="">
												</a>
												<div class="content">
													<h3>Convergence of CMA-ES</h3>
													<p><a href="https://cjcma2023.sciencesconf.org/" target=”_blank”>CJC-MA 2023</a>, September 2023, CentraleSupélec (France)</p>
												</div>
											</article>
											<article>
												<a href="presentation_gecco23.pdf" target=”_blank” class="image">
													<img width="50" src="images/benchmark.png" alt="">
												</a>
												<div class="content">
													<h3>Evaluation of the impact of various modifications to CMA-ES that facilitate its theoretical analysis</h3>
													<p><a href="https://gecco-2023.sigevo.org/HomePage" target=”_blank”>GECCO '23</a>, July 2023, Lisbon (Portugal)</p>
												</div>
											</article>
											<article>
												<a href="GISSLER_SIAMOP.pdf" target=”_blank” class="image">
													<img src="images/drift_arrow-1.png" alt="">
												</a>
												<div class="content">
													<h3>Convergence Analysis of CMA-ES via Markov Chain Stability Analysis</h3>
													<p><a href="https://www.siam.org/conferences/cm/conference/op23" target=”_blank”>SIAM OP23</a>, June 2023, Seattle (US)</p>
												</div>
											</article>
											<article>
												<a href="presentation-line-search.pdf" target=”_blank” class="image object">
													<img width="350" src="images/linesearch4.png" alt="" />
													<img width="350" src="images/linesearch8.png" alt="" />
												</a>
												<div class="content">
													<h3>Learning Rate Adaptation by Line Search in Evolution Strategies</h3>
													<p><a href="https://gecco-2022.sigevo.org/HomePage" target=”_blank”>GECCO '22</a>, July 2022, Boston (US)</p>
												</div>
											</article>
											<article>
											</article>
										</div>
									</div>										
								</section>
							

								
								


							<!-- Teaching -->
								<section id="teach">
									<div class="content">
										<header class="major">
											<h2>Teaching</h2>
										</header>
										<p>Bachelor of Science in  École polytechnique, Teaching assistant for LAB 102 - How to write mathematics: <a href="https://moodle.polytechnique.fr/course/view.php?id=17559" target=”_blank”>2023-2024</a>, <a href="https://moodle.polytechnique.fr/course/view.php?id=14922" target=”_blank”>2022-2023</a>, <a href="https://moodle.polytechnique.fr/enrol/index.php?id=13159" target=”_blank”>2021-2022</a></li>
									</div>
								</section>
							
							<!-- Misc -->
								<section id="misc">
									<div class="content">
										<header class="major">
											<h2>Misc.</h2>
										</header>
										<p>2023: Reviewer for <a href="https://gecco-2023.sigevo.org/HomePage" target=”_blank”>GECCO '23</a></p>
										<p>2022: Reviewer for <a href="https://www.springer.com/journal/11228" target=”_blank”>Set-Valued and Variational Analysis</a></p>
										<p>2022-2024: Member of the Laboratory life commission (CMAP)</p>
										<p>2022-2024: Organization of the <a href="https://cmap.ip-paris.fr/seminaires/seminaire-des-doctorants" target=”_blank”>CMAP-CMLS PhD students seminar</a></p>
										<p>2022: Editorial assistant for Dagstuhl Seminar <a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/22081" target=”_blank”>Theory of Randomized Optimization Heuristics</a></p>
									</div>
								</section>
								
								

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<ul>
										<li><a href="#header">Home</a></li>
										<!--<li><a href="#res_int">Research interests</a></li>-->
										<li><a href="#publis">Publications</a></li>
										<li><a href="#confs">Communications</a></li>
										<li><a href="#teach">Teaching</a></li>
										<li><a href="#misc">Misc.</a></li>
									</ul>
								</nav>
							
							

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Contact</h2>
									</header>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="mailto:firstname.lastname[at]inria[dot]fr">first.last[at]inria[dot]fr</a></li>
									</ul>
								</section>
								<span class="image"><img height="100" src="images/moi.jpeg" alt=""></span>

							

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
